{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b362652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CNN-LSTM 회귀 학습 전용 스크립트\n",
    "# - 입력  C:\\ESG_Project1\\file\\solar_data_file\\남동발전량_지역매핑_시간행_기상병합.csv\n",
    "# - 타깃  \"발전량(MWh)\" 기본값\n",
    "# - 출력  같은 폴더에 cnn_lstm_solar.pt 저장\n",
    "# - 기능  범주형 원핫 숫자 표준화 시계열 윈도우 시간순 분할 조기 종료\n",
    "# ================================\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ===== 사용자 설정 =====\n",
    "CSV_PATH = r\"C:\\ESG_Project1\\file\\solar_data_file\\남동발전량_지역매핑_시간행_기상병합.csv\"\n",
    "TARGET   = \"발전량(MWh)\"       # 타깃 열 이름\n",
    "SEQ_LEN  = 24                  # 입력 시퀀스 길이\n",
    "HORIZON  = 1                   # 예측 지평 1시간\n",
    "BATCH    = 256\n",
    "EPOCHS   = 200\n",
    "LR       = 1e-3\n",
    "VAL_RATIO = 0.1                # 마지막 10%를 검증으로 사용\n",
    "PATIENCE = 20                  # 조기 종료 인내\n",
    "NUM_WORKERS = 0                # Windows 환경 안전값\n",
    "\n",
    "USE_GPU  = torch.cuda.is_available()\n",
    "AMP      = True                # 혼합정밀 학습 사용\n",
    "MODEL_DIR = os.path.dirname(CSV_PATH)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"cnn_lstm_solar.pt\")\n",
    "\n",
    "# ===== 데이터 적재 및 전처리 =====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Datetime 열 자동 탐지\n",
    "dt_col = None\n",
    "cands = [c for c in df.columns if any(k in str(c).lower() for k in [\"datetime\",\"일시\",\"시간\",\"timestamp\",\"ts\",\"측정시각\"])]\n",
    "if cands:\n",
    "    dt_col = cands[0]\n",
    "    try:\n",
    "        df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "        df = df.sort_values(dt_col).reset_index(drop=True)\n",
    "    except Exception:\n",
    "        dt_col = None\n",
    "\n",
    "if dt_col is None:\n",
    "    # 일자 + 시 조합\n",
    "    date_like = [c for c in df.columns if any(k in str(c).lower() for k in [\"일자\",\"날짜\",\"date\"])]\n",
    "    hour_like = [c for c in df.columns if any(k in str(c).lower() for k in [\"시\",\"hour\",\"시간\"])]\n",
    "    if date_like and hour_like:\n",
    "        dcol, hcol = date_like[0], hour_like[0]\n",
    "        df[\"__dt__\"] = pd.to_datetime(df[dcol]) + pd.to_timedelta(df[hcol].astype(int), unit=\"h\")\n",
    "        dt_col = \"__dt__\"\n",
    "        df = df.sort_values(dt_col).reset_index(drop=True)\n",
    "\n",
    "# 시간 파생 특성\n",
    "time_feats = []\n",
    "if dt_col is not None:\n",
    "    df[\"hour\"]      = df[dt_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[dt_col].dt.dayofweek\n",
    "    df[\"month\"]     = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"]= (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    time_feats = [\"hour\",\"dayofweek\",\"month\",\"is_weekend\"]\n",
    "\n",
    "# 타깃 확인\n",
    "if TARGET not in df.columns:\n",
    "    num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    assert len(num_cols_all) > 0, \"숫자형 타깃 후보가 없습니다. TARGET 이름을 확인하세요.\"\n",
    "    TARGET = num_cols_all[-1]\n",
    "\n",
    "# 학습에 불필요한 열 제거\n",
    "drop_cols = []\n",
    "for c in df.columns:\n",
    "    if \"Unnamed\" in str(c) or \"index\" == str(c).lower() or \"id\" == str(c).lower():\n",
    "        drop_cols.append(c)\n",
    "if dt_col is not None:\n",
    "    drop_cols.append(dt_col)\n",
    "\n",
    "# 범주형 원핫\n",
    "cat_cols = df.drop(columns=[TARGET] + drop_cols, errors=\"ignore\").select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "df_cat = pd.get_dummies(df[cat_cols].fillna(\"MISSING\"), dummy_na=False) if cat_cols else pd.DataFrame(index=df.index)\n",
    "\n",
    "# 수치 컬럼\n",
    "num_cols = df.drop(columns=[TARGET] + drop_cols + cat_cols, errors=\"ignore\").select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# 최종 특성 행렬\n",
    "X_df = pd.concat([df[num_cols], df_cat, df[time_feats]] if time_feats else [df[num_cols], df_cat], axis=1)\n",
    "y_df = df[TARGET].astype(float)\n",
    "\n",
    "# 시간순 분할 인덱스\n",
    "n = len(X_df)\n",
    "assert n > SEQ_LEN + HORIZON, \"시퀀스 길이보다 데이터가 더 필요합니다.\"\n",
    "split_idx = int(n * (1 - VAL_RATIO))\n",
    "X_train_df, X_val_df = X_df.iloc[:split_idx].copy(), X_df.iloc[split_idx:].copy()\n",
    "y_train,   y_val     = y_df.iloc[:split_idx].copy(), y_df.iloc[split_idx:].copy()\n",
    "\n",
    "# 스케일러는 훈련 구간에만 적합\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_df.values)\n",
    "X_val   = scaler.transform(X_val_df.values)\n",
    "\n",
    "feature_dim = X_train.shape[1]\n",
    "\n",
    "# ===== PyTorch 데이터셋 =====\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int, horizon: int):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "        self.h = horizon\n",
    "        self.max_i = len(X) - seq_len - horizon + 1\n",
    "        self.max_i = max(self.max_i, 0)\n",
    "    def __len__(self):\n",
    "        return self.max_i\n",
    "    def __getitem__(self, idx: int):\n",
    "        x_seq = self.X[idx: idx + self.seq_len]                  # [L, F]\n",
    "        y_tgt = self.y[idx + self.seq_len + self.h - 1]          # 스칼라\n",
    "        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_tgt, dtype=torch.float32)\n",
    "\n",
    "train_ds = SeqDataset(X_train, y_train.values, SEQ_LEN, HORIZON)\n",
    "val_ds   = SeqDataset(X_val,   y_val.values,   SEQ_LEN, HORIZON)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=USE_GPU)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=USE_GPU)\n",
    "\n",
    "# ===== 모델 정의 =====\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, in_features: int, seq_len: int,\n",
    "                 conv_channels: int = 64, lstm_hidden: int = 128, lstm_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        # 입력 [B, L, F] → Conv1d는 [B, C, L] 형식이므로 채널을 F로 보고 시퀀스 축을 길이로 사용\n",
    "        self.bn_in = nn.BatchNorm1d(in_features)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_features, conv_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=seq_len // 2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden,\n",
    "                            num_layers=lstm_layers, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):                       # x [B, L, F]\n",
    "        x = x.transpose(1, 2)                  # [B, F, L]\n",
    "        x = self.bn_in(x)                      # [B, F, L]\n",
    "        x = self.conv(x)                       # [B, C, L']\n",
    "        x = x.transpose(1, 2)                  # [B, L', C]\n",
    "        out, _ = self.lstm(x)                  # [B, L', H]\n",
    "        out = out[:, -1, :]                    # [B, H]\n",
    "        y = self.head(out).squeeze(-1)         # [B]\n",
    "        return y\n",
    "\n",
    "device = torch.device(\"cuda\" if USE_GPU else \"cpu\")\n",
    "model = CNNLSTM(in_features=feature_dim, seq_len=SEQ_LEN).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.L1Loss()  # MAE 기반 학습. 필요 시 nn.MSELoss 로 교체\n",
    "scaler_t = torch.cuda.amp.GradScaler(enabled=AMP and USE_GPU)\n",
    "\n",
    "# ===== 학습 루프  평가 출력 없음  조기 종료만 =====\n",
    "best_val = math.inf\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=AMP and USE_GPU):\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "        scaler_t.scale(loss).backward()\n",
    "        scaler_t.step(optimizer)\n",
    "        scaler_t.update()\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 손실만 계산  출력은 하지 않음\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP and USE_GPU):\n",
    "                pred = model(xb)\n",
    "                l = criterion(pred, yb).item()\n",
    "            val_loss += l * xb.size(0)\n",
    "            n_val += xb.size(0)\n",
    "    val_loss /= max(n_val, 1)\n",
    "\n",
    "    # 조기 종료\n",
    "    if val_loss + 1e-8 < best_val:\n",
    "        best_val = val_loss\n",
    "        wait = 0\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"scaler_mean\": scaler.mean_.astype(np.float32),\n",
    "            \"scaler_scale\": scaler.scale_.astype(np.float32),\n",
    "            \"feature_columns\": X_df.columns.tolist(),\n",
    "            \"seq_len\": SEQ_LEN,\n",
    "            \"horizon\": HORIZON,\n",
    "            \"in_features\": feature_dim,\n",
    "        }, MODEL_PATH)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            break\n",
    "\n",
    "print(f\"✅ 모델 저장 완료 → {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
